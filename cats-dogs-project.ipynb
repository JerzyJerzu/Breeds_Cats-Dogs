{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":325302,"sourceType":"datasetVersion","datasetId":137362},{"sourceId":7520479,"sourceType":"datasetVersion","datasetId":4380888}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"f5b41e5a-0d11-449c-a5c9-477b57a65a9a","_cell_guid":"f05ac9cf-11a9-46d6-95a7-baf6d0ac38ee","trusted":true}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"0d7d39d3-a752-4ac3-a9f1-da293f88a4c9","_cell_guid":"a8c5fae8-0f13-4278-9a41-2935660089ba","execution":{"iopub.status.busy":"2024-02-08T14:20:35.854904Z","iopub.execute_input":"2024-02-08T14:20:35.855876Z","iopub.status.idle":"2024-02-08T14:20:36.214536Z","shell.execute_reply.started":"2024-02-08T14:20:35.855833Z","shell.execute_reply":"2024-02-08T14:20:36.213773Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import datasets, transforms\nimport torchvision.models as models\nfrom datetime import datetime\nimport wandb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nrandom_state = 2137\nnp.random.seed(random_state)\ntorch.manual_seed(random_state)\ntorch.cuda.manual_seed(random_state)","metadata":{"_uuid":"dd292565-8513-47b8-a2c6-7a4977473de4","_cell_guid":"6fafdea2-41e8-4cf1-9a6a-f64db210ca65","execution":{"iopub.status.busy":"2024-02-08T14:20:36.215980Z","iopub.execute_input":"2024-02-08T14:20:36.216466Z","iopub.status.idle":"2024-02-08T14:20:40.791276Z","shell.execute_reply.started":"2024-02-08T14:20:36.216440Z","shell.execute_reply":"2024-02-08T14:20:40.790518Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = 'cac5e1e8113d6c2054d4bc95b0b518086be2b55d'","metadata":{"_uuid":"8d04d5bc-4435-4e39-b749-a60ca5a0d63e","_cell_guid":"d720493f-0c28-4daa-b44d-924b21a078fa","execution":{"iopub.status.busy":"2024-02-08T14:20:40.792378Z","iopub.execute_input":"2024-02-08T14:20:40.792663Z","iopub.status.idle":"2024-02-08T14:20:40.796987Z","shell.execute_reply.started":"2024-02-08T14:20:40.792638Z","shell.execute_reply":"2024-02-08T14:20:40.796164Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Initialising the project on wandb","metadata":{"_uuid":"54ff73d3-4361-4fb4-aa49-47982d5f86a4","_cell_guid":"49971fda-7849-46de-a925-f87b76c08dfc","trusted":true}},{"cell_type":"code","source":"def initialize_wandb():\n    current_time = datetime.now().strftime(\"%m-%d_%H:%M\")\n    #info = my_config.get_configuration_info()\n    run_name = f\"tuning_inception_{current_time}\"\n    #print(run_name)\n    wandb.init()\n    #wandb.init(project='cats&dogs_ML&DL_project', save_code=True, config = sweep_config, name = run_name)\n    return","metadata":{"_uuid":"40e08122-5bde-451d-8a5d-1efc92f138a3","_cell_guid":"624d8b13-c4e8-47cd-aaa0-136330fd2c22","execution":{"iopub.status.busy":"2024-02-08T14:20:40.799335Z","iopub.execute_input":"2024-02-08T14:20:40.799944Z","iopub.status.idle":"2024-02-08T14:20:40.809016Z","shell.execute_reply.started":"2024-02-08T14:20:40.799913Z","shell.execute_reply":"2024-02-08T14:20:40.808265Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def initialize_wandb_with_config(config):\n    current_time = datetime.now().strftime(\"%m-%d_%H:%M\")\n    #info = my_config.get_configuration_info()\n    run_name = f\"training{current_time}\"\n    #print(run_name)\n    #wandb.init()\n    wandb.init(project='cats&dogs_ML&DL_project', save_code=True, config = config, name = run_name)\n    return","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:20:40.809981Z","iopub.execute_input":"2024-02-08T14:20:40.810251Z","iopub.status.idle":"2024-02-08T14:20:40.820261Z","shell.execute_reply.started":"2024-02-08T14:20:40.810230Z","shell.execute_reply":"2024-02-08T14:20:40.819384Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Parameters","metadata":{"_uuid":"d9b755a5-9bed-4d0c-9379-ada101ebb874","_cell_guid":"60f41da9-366c-4dd9-bff6-9f915e9f23db","trusted":true}},{"cell_type":"code","source":"class configuration:\n    def __init__(self, sweep_config, model, model_transforms, last_checkpoint_path=None):\n        config = wandb.config\n        #config = wandb.init().config\n        print('configuraiton class ',config)\n        self.last_checkpoint_path = last_checkpoint_path\n        self.model = model\n        print(\"loading parameters\")\n        self.batch_size = config.batch_size\n        print(\"batch\")\n        self.init_learning_rate = config.learning_rate\n        print(\"lr\")\n        self.optimizer_name = config.optimizer\n        print(\"optimizer\")\n        print(\"loaded parameters\")\n        self.optimizer = self.initialize_optimizer()\n        print(\"got optimizer\")\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience=3, verbose=True, min_lr=self.init_learning_rate/32, factor=0.5)\n        self.criterion = nn.CrossEntropyLoss()\n        \n        self.test_transforms = model_transforms\n        # combine with some basic transforms\n        self.custom_transforms = get_costum_transforms()\n        self.train_transforms = transforms.Compose([self.custom_transforms,model_transforms])\n    def initialize_optimizer(self):\n        print(\"getting optimizer\")\n        if self.optimizer_name == 'sgd':\n            optimizer = optim.SGD(self.model.parameters(), lr=self.init_learning_rate, momentum=0.9)\n            print('sgd')\n        elif self.optimizer_name == 'adam':\n            optimizer = optim.Adam(self.model.parameters(), lr=self.init_learning_rate)\n            print('adam')\n        else:\n            print(\"Unsupported optimizer:\")\n            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n        return optimizer\n    def get_configuration_info(self):\n        info = f\"{type(self.model).__name__}_\"\n        info += f\"{type(self.optimizer).__name__}_\"\n        info += f\"Batch={self.batch_size}_\"\n        info += f\"lr={self.optimizer.param_groups[0]['lr']}\"\n        return info\n    def get_configuration_dictionary(self):\n        configuration_info = {\n            'model_type': type(self.model).__name__,\n            'criterion': str(self.criterion),\n            'optimizer': type(self.optimizer).__name__,\n            'batch_size': self.batch_size,\n            'scheduler': type(self.scheduler).__name__,\n            'optimizer_params': {\n                'initial_lr': self.optimizer.param_groups[0]['lr']\n            },\n            'scheduler_params': {\n                'min_lr': self.scheduler.min_lrs,\n                'patience': self.scheduler.patience,\n                'factor': self.scheduler.factor\n            },\n            'custom_transforms': [str(transform) for transform in self.custom_transforms.transforms] if self.custom_transforms else None\n        }\n\n        return configuration_info","metadata":{"_uuid":"07422de6-5aab-495c-b98c-a5d8339a27af","_cell_guid":"46ec50f0-1641-4f1b-8c0e-af6d444070cd","_kg_hide-output":false,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-02-08T14:20:40.821379Z","iopub.execute_input":"2024-02-08T14:20:40.821676Z","iopub.status.idle":"2024-02-08T14:20:40.835980Z","shell.execute_reply.started":"2024-02-08T14:20:40.821648Z","shell.execute_reply":"2024-02-08T14:20:40.835088Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# loading datasets","metadata":{"_uuid":"3dcf512b-4e5c-413c-aad3-0028f8be4fe0","_cell_guid":"fd3174c1-4aee-4e29-bf1c-2c8b175f5bc8","trusted":true}},{"cell_type":"code","source":"#reading csv\nannotations = pd.read_csv('../input/cats-and-dogs-breeds-classification-oxford-dataset/annotations/annotations/list.txt')\n\n#The first 4 rows consists of the information about breeds\n#Reading the data after 5th row\nannotations = annotations.loc[5:,]\n\n#Processing the columns\nannotations[['CLASS-ID','SPECIES','BREED','ID']] = annotations['#Image CLASS-ID SPECIES BREED ID'].str.split(expand=True) \n\n#Dropping unnecessary columns\nannotations = annotations.drop('#Image CLASS-ID SPECIES BREED ID',axis=1)\n\n#renaming the columns\nannotations = annotations.rename(columns={\"CLASS-ID\": \"image\", \"SPECIES\": \"CLASS-ID\", 'BREED' : \"SPECIES\", \"ID\":\"BREED ID\"})\n\n\n#converting the object type to int type\nannotations[[\"CLASS-ID\",\"SPECIES\",\"BREED ID\"]] = annotations[[\"CLASS-ID\",\"SPECIES\",\"BREED ID\"]].astype(int)","metadata":{"_uuid":"c83b9513-15a6-426a-bcb4-9dd800a72898","_cell_guid":"d1d10af1-daa4-443b-87ab-1694cd1d502d","execution":{"iopub.status.busy":"2024-02-08T14:20:40.837051Z","iopub.execute_input":"2024-02-08T14:20:40.837328Z","iopub.status.idle":"2024-02-08T14:20:40.915573Z","shell.execute_reply.started":"2024-02-08T14:20:40.837306Z","shell.execute_reply":"2024-02-08T14:20:40.914446Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"annotations","metadata":{"_uuid":"b4552eef-311b-4783-a6f2-d72d38ffa8ad","_cell_guid":"44213b49-56b3-42e6-b7e5-4b55d94f2e43","execution":{"iopub.status.busy":"2024-02-08T14:20:40.916871Z","iopub.execute_input":"2024-02-08T14:20:40.917219Z","iopub.status.idle":"2024-02-08T14:20:40.934539Z","shell.execute_reply.started":"2024-02-08T14:20:40.917191Z","shell.execute_reply":"2024-02-08T14:20:40.933439Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                     image  CLASS-ID  SPECIES  BREED ID\n5           Abyssinian_100         1        1         1\n6           Abyssinian_101         1        1         1\n7           Abyssinian_102         1        1         1\n8           Abyssinian_103         1        1         1\n9           Abyssinian_104         1        1         1\n...                    ...       ...      ...       ...\n7349  yorkshire_terrier_96        37        2        25\n7350  yorkshire_terrier_97        37        2        25\n7351  yorkshire_terrier_98        37        2        25\n7352  yorkshire_terrier_99        37        2        25\n7353   yorkshire_terrier_9        37        2        25\n\n[7349 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>CLASS-ID</th>\n      <th>SPECIES</th>\n      <th>BREED ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>Abyssinian_100</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Abyssinian_101</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Abyssinian_102</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Abyssinian_103</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Abyssinian_104</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7349</th>\n      <td>yorkshire_terrier_96</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>7350</th>\n      <td>yorkshire_terrier_97</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>7351</th>\n      <td>yorkshire_terrier_98</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>7352</th>\n      <td>yorkshire_terrier_99</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>7353</th>\n      <td>yorkshire_terrier_9</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n<p>7349 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Species stands here for dog (2) or a cat (1), breed ID is the ID of a breed given we know what type of animal it is, and class-ID is a unique ID for each species and breed together. Overall there is 25 cats breeds and 12 dog breeds. image is a column of filenames. All the files here are in jpg format","metadata":{"_uuid":"06ab5418-056c-414c-bf79-c43ee63882f3","_cell_guid":"2d64c844-43a0-43d1-ada8-15f49bc395c8","trusted":true}},{"cell_type":"markdown","source":"### RUN THIS CELL ONLY ONCE!!!","metadata":{"_uuid":"5d3184df-d2c8-4cda-8b1b-7784f9c6253e","_cell_guid":"25c82c4e-bf8c-4e06-af65-7b43aaf665c6","trusted":true}},{"cell_type":"code","source":"# adding the extension to image so it can be used to access the real image\nannotations['image'] = annotations['image'].apply(lambda x : str(x)+'.jpg')\nannotations = annotations.reset_index()\nannotations = annotations.drop('index',axis=1)\n\n#Extracting the classname/breed of the animal\nannotations['classname'] = annotations['image'].apply(lambda x: str(x)[:str(x).rindex('_')])\n\n# Adding information about cat or dog based on the 'Species' column to the 'classname' column\nannotations['classname'] = annotations.apply(lambda row: f\"{('dog' if row['SPECIES'] == 2 else 'cat')}_{row['classname']}\", axis=1)\nannotations","metadata":{"_uuid":"a65a197e-9037-43a1-b283-e715416793a1","_cell_guid":"ddc198d8-0457-4df8-9ae9-1c5b81591540","execution":{"iopub.status.busy":"2024-02-08T14:20:40.935957Z","iopub.execute_input":"2024-02-08T14:20:40.936348Z","iopub.status.idle":"2024-02-08T14:20:41.068857Z","shell.execute_reply.started":"2024-02-08T14:20:40.936318Z","shell.execute_reply":"2024-02-08T14:20:41.068020Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                         image  CLASS-ID  SPECIES  BREED ID  \\\n0           Abyssinian_100.jpg         1        1         1   \n1           Abyssinian_101.jpg         1        1         1   \n2           Abyssinian_102.jpg         1        1         1   \n3           Abyssinian_103.jpg         1        1         1   \n4           Abyssinian_104.jpg         1        1         1   \n...                        ...       ...      ...       ...   \n7344  yorkshire_terrier_96.jpg        37        2        25   \n7345  yorkshire_terrier_97.jpg        37        2        25   \n7346  yorkshire_terrier_98.jpg        37        2        25   \n7347  yorkshire_terrier_99.jpg        37        2        25   \n7348   yorkshire_terrier_9.jpg        37        2        25   \n\n                  classname  \n0            cat_Abyssinian  \n1            cat_Abyssinian  \n2            cat_Abyssinian  \n3            cat_Abyssinian  \n4            cat_Abyssinian  \n...                     ...  \n7344  dog_yorkshire_terrier  \n7345  dog_yorkshire_terrier  \n7346  dog_yorkshire_terrier  \n7347  dog_yorkshire_terrier  \n7348  dog_yorkshire_terrier  \n\n[7349 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>CLASS-ID</th>\n      <th>SPECIES</th>\n      <th>BREED ID</th>\n      <th>classname</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Abyssinian_100.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>cat_Abyssinian</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Abyssinian_101.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>cat_Abyssinian</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Abyssinian_102.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>cat_Abyssinian</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Abyssinian_103.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>cat_Abyssinian</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abyssinian_104.jpg</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>cat_Abyssinian</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7344</th>\n      <td>yorkshire_terrier_96.jpg</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n      <td>dog_yorkshire_terrier</td>\n    </tr>\n    <tr>\n      <th>7345</th>\n      <td>yorkshire_terrier_97.jpg</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n      <td>dog_yorkshire_terrier</td>\n    </tr>\n    <tr>\n      <th>7346</th>\n      <td>yorkshire_terrier_98.jpg</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n      <td>dog_yorkshire_terrier</td>\n    </tr>\n    <tr>\n      <th>7347</th>\n      <td>yorkshire_terrier_99.jpg</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n      <td>dog_yorkshire_terrier</td>\n    </tr>\n    <tr>\n      <th>7348</th>\n      <td>yorkshire_terrier_9.jpg</td>\n      <td>37</td>\n      <td>2</td>\n      <td>25</td>\n      <td>dog_yorkshire_terrier</td>\n    </tr>\n  </tbody>\n</table>\n<p>7349 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Adding transformations","metadata":{"_uuid":"5de72031-2f24-4082-add2-78f5b2997138","_cell_guid":"e4c62191-d037-479d-8003-966d214df6bf","trusted":true}},{"cell_type":"code","source":"def get_costum_transforms():\n    costum_transforms = torchvision.transforms.Compose([\n        transforms.RandomRotation(degrees=(-30, 30),fill=None),\n        transforms.Resize((300,300)),\n        transforms.RandomApply([transforms.Compose([\n                transforms.CenterCrop(200),\n                transforms.RandomCrop(80),\n            ]),], p=0.3),\n        transforms.RandomHorizontalFlip(p=0.4),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n        transforms.GaussianBlur(kernel_size=(5,5), sigma=0.3),\n        transforms.RandomApply([transforms.Compose([\n                transforms.GaussianBlur(kernel_size=(9,9), sigma=0.7),\n            ]),], p=0.4),\n      ])\n    return costum_transforms","metadata":{"_uuid":"014a9871-c116-4325-84d9-11d993cc832f","_cell_guid":"1e8ce57e-9537-46bd-93bd-6d48f4d05a30","execution":{"iopub.status.busy":"2024-02-08T14:20:41.073124Z","iopub.execute_input":"2024-02-08T14:20:41.073404Z","iopub.status.idle":"2024-02-08T14:20:41.080204Z","shell.execute_reply.started":"2024-02-08T14:20:41.073381Z","shell.execute_reply":"2024-02-08T14:20:41.079257Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"\n\n# Creating the dataset class to make it easily accessable and short data analysis","metadata":{"_uuid":"bc104db6-a1f3-4a07-941f-e6dd9a5683f0","_cell_guid":"01439d6b-2580-4a84-954a-b0e25a369a2a","trusted":true}},{"cell_type":"code","source":"class CatsDogsDataset(Dataset):\n    def __init__(self, file_list, transform=None):\n        self.folder_patch = '/kaggle/input/cats-and-dogs-breeds-classification-oxford-dataset/images/images/'\n        self.annotations = file_list\n        self.transform = transform\n        self.filelength = len(file_list)\n\n    def __len__(self):\n        return self.filelength\n\n    def __getitem__(self, idx):\n        classID = self.annotations['CLASS-ID'].iloc[idx]\n        img_path = self.annotations['image'].iloc[idx]\n        img_path = self.folder_patch + img_path\n        img = Image.open(img_path)\n        \n        has_alpha_channel = img.mode == 'RGBA'\n        if has_alpha_channel == True:\n            print(\"image has Alpha channel\")\n            img = img.convert('RGB')\n        if self.transform is not None:\n            try:\n                img = self.transform(img)\n            except RuntimeError as e:\n                print(f\"Exception: {e}\")\n                print(\"Shape before normalization:\", img.size)\n                print(img_path)\n                tot = transforms.ToTensor()\n                img_tensor = tot(img)\n                print(\"Input Tensor Shape:\", img_tensor.shape)\n                print(\"Input Tensor Values:\", img_tensor)\n        #else:\n            #print(\"No transformations to be done\")\n        return img, classID-1","metadata":{"_uuid":"678f73e2-736c-4b98-a6c4-d0084ff8955d","_cell_guid":"dc8cbeab-2d61-4464-be7e-ea5998ea135e","execution":{"iopub.status.busy":"2024-02-08T14:20:41.081166Z","iopub.execute_input":"2024-02-08T14:20:41.081461Z","iopub.status.idle":"2024-02-08T14:20:41.092783Z","shell.execute_reply.started":"2024-02-08T14:20:41.081437Z","shell.execute_reply":"2024-02-08T14:20:41.092076Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the histogram of images resolutions (I couldn't find any information about the images resolutions in a dataset)","metadata":{"_uuid":"727204d3-5ac6-4d85-8aae-ca7de7134422","_cell_guid":"06a3ed58-05c3-4bae-9846-f50508e0368d","trusted":true}},{"cell_type":"code","source":"all_images = CatsDogsDataset(annotations)","metadata":{"_uuid":"ca49b3cf-0779-4055-91f0-94e64feee06c","_cell_guid":"807c483d-7f9d-4b67-8537-178e9c284178","execution":{"iopub.status.busy":"2024-02-08T14:20:41.093758Z","iopub.execute_input":"2024-02-08T14:20:41.094079Z","iopub.status.idle":"2024-02-08T14:20:41.106310Z","shell.execute_reply.started":"2024-02-08T14:20:41.094049Z","shell.execute_reply":"2024-02-08T14:20:41.105461Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"dataset_size = len(all_images)\nprint(dataset_size)\nimage_sizes = []\nfor i in range(dataset_size):\n    img, _ = all_images[i]\n    image_sizes.append(img.size)","metadata":{"_uuid":"d8a39fa8-937e-4c17-b031-013fa883315d","_cell_guid":"68f4e428-2a28-47dd-a3d2-c45134509a4b","execution":{"iopub.status.busy":"2024-01-27T12:05:45.472305Z","iopub.execute_input":"2024-01-27T12:05:45.472759Z","iopub.status.idle":"2024-01-27T12:05:55.092793Z","shell.execute_reply.started":"2024-01-27T12:05:45.472702Z","shell.execute_reply":"2024-01-27T12:05:55.091935Z"},"trusted":true}},{"cell_type":"markdown","source":"unique_count = len(set(image_sizes))\nprint(\"Number of unique elements:\", unique_count)","metadata":{"_uuid":"f3f0e2f8-9298-493b-9574-ce937b10ce01","_cell_guid":"65c37f27-3a47-482f-9736-e9edd6363542","execution":{"iopub.status.busy":"2024-01-27T12:05:55.094222Z","iopub.execute_input":"2024-01-27T12:05:55.094919Z","iopub.status.idle":"2024-01-27T12:05:55.103210Z","shell.execute_reply.started":"2024-01-27T12:05:55.094877Z","shell.execute_reply":"2024-01-27T12:05:55.101839Z"},"trusted":true}},{"cell_type":"markdown","source":"x_values, y_values = zip(*image_sizes)\nplt.title('x_values')\nplt.hist(x_values,bins=100)\nplt.show()\nplt.title('y_values')\nplt.hist(y_values,bins=100)\nplt.show()","metadata":{"_uuid":"9a282160-4ece-42b9-a511-559618624f01","_cell_guid":"5c42afe9-b4a1-4671-b9b2-1612329b7b77","execution":{"iopub.status.busy":"2024-01-27T12:05:55.109600Z","iopub.execute_input":"2024-01-27T12:05:55.110230Z","iopub.status.idle":"2024-01-27T12:05:56.106594Z","shell.execute_reply.started":"2024-01-27T12:05:55.110192Z","shell.execute_reply":"2024-01-27T12:05:56.105464Z"},"trusted":true}},{"cell_type":"markdown","source":"resoulutions above 1000 pixels in one axis are not representative","metadata":{"_uuid":"41fea3a9-ea64-4c21-8a4f-0f1d042dc778","_cell_guid":"8ae27c5c-f2bf-4a43-8ef3-43350b9d7b87","trusted":true}},{"cell_type":"markdown","source":"filtered_data = [(x, y) for x, y in image_sizes if x < 1000 and y < 1000]\n\nx_values, y_values = zip(*filtered_data)\nplt.hist2d(x_values, y_values, bins=(50, 50), cmap='viridis', cmin = 1)\n\nplt.xlim(0, 1000)\nplt.ylim(0, 1000)\n\n# Add color bar for reference\ncbar = plt.colorbar()\ncbar.set_label('Frequency')\n\n# Add labels and title\nplt.xlabel('horizontal')\nplt.ylabel('vertical')\nplt.title('2D Histogram for resolutions')\n\n# Show the plot\nplt.show()","metadata":{"_uuid":"783f1057-b2b7-43a7-ab2a-2a9aa2acb8ac","_cell_guid":"cccd37a9-37fd-41af-801e-b56c822156f9","execution":{"iopub.status.busy":"2024-01-27T12:05:56.108268Z","iopub.execute_input":"2024-01-27T12:05:56.108626Z","iopub.status.idle":"2024-01-27T12:05:56.462920Z","shell.execute_reply.started":"2024-01-27T12:05:56.108593Z","shell.execute_reply":"2024-01-27T12:05:56.461804Z"},"trusted":true}},{"cell_type":"markdown","source":"Most of the images have rectangular shapes with proportions around 5x4 or 3x5 which should be also true in real life scenarions","metadata":{"_uuid":"879efffa-a47c-4889-af60-6f71cdf90ca8","_cell_guid":"730f952f-4d66-4802-8197-be335f662bdd","trusted":true}},{"cell_type":"markdown","source":"# Create a 2D histogram\nhist, x_edges, y_edges, _ = plt.hist2d(x_values, y_values, bins=(30, 30), cmap='inferno', cmin = 1, vmax=30)\n\nplt.xlim(0, 1000)\nplt.ylim(0, 1000)\n\n# Add color bar for reference\ncbar = plt.colorbar()\ncbar.set_label('Frequency')\n\ncbar.set_ticks([0, 2, 4, 6, 10, 20, 30])\n\nplt.xlabel('horizontal')\nplt.ylabel('vertical')\nplt.title('2D Histogram for rare resolutions')\nplt.show()","metadata":{"_uuid":"8d76b822-8f59-4589-8eb7-72ab7013d605","_cell_guid":"cb95b86f-69dd-491d-84f0-454d09291d4d","execution":{"iopub.status.busy":"2024-01-27T12:05:56.464376Z","iopub.execute_input":"2024-01-27T12:05:56.465377Z","iopub.status.idle":"2024-01-27T12:05:56.785870Z","shell.execute_reply.started":"2024-01-27T12:05:56.465338Z","shell.execute_reply":"2024-01-27T12:05:56.784916Z"},"trusted":true}},{"cell_type":"markdown","source":"It can be obserwed that resolutions below 250 and 550 pixels in any axis also appear rarely","metadata":{"_uuid":"ee6063bb-c5f5-4207-85b5-8f2a41e27d8e","_cell_guid":"c2e374f0-f5bf-4651-90f3-b8c2c625e1d4","trusted":true}},{"cell_type":"markdown","source":"### Plotting the distribution of animal breed to check if they are really distributed uniformally","metadata":{"_uuid":"a680cf48-7bc2-4f63-9d39-48e23752ef7d","_cell_guid":"eae9bbf3-79dd-403b-9fe2-b2e1185a48fd","trusted":true}},{"cell_type":"markdown","source":"plt.hist(annotations['classname'], bins=37, edgecolor='black',rwidth=0.5)\nplt.xticks(rotation='vertical')\nplt.xlabel('breed indexes')\nplt.ylabel('Frequency')\nplt.title('Frequency of different breeds in dataset')\nplt.show()","metadata":{"_uuid":"a868c7c5-8010-480d-9cbf-fc55ae19c906","_cell_guid":"0e9c71a6-1c1d-4c01-8a75-b86a7c532c28","execution":{"iopub.status.busy":"2024-01-27T12:05:56.787317Z","iopub.execute_input":"2024-01-27T12:05:56.787684Z","iopub.status.idle":"2024-01-27T12:05:57.338815Z","shell.execute_reply.started":"2024-01-27T12:05:56.787650Z","shell.execute_reply":"2024-01-27T12:05:57.337612Z"},"trusted":true}},{"cell_type":"markdown","source":"The breeds are actually almost uniformally distributed","metadata":{"_uuid":"6affdf19-29e7-4ccd-b58c-c93b6855b8f4","_cell_guid":"ad31c515-9a31-4735-9aa4-14d81796cde8","trusted":true}},{"cell_type":"markdown","source":"# Splitting datasets and creating the dataloaders","metadata":{"_uuid":"9efe1d59-3943-4ea5-ab01-0d8cfea4ca3d","_cell_guid":"b6fed058-bdf1-4cb9-a666-6801541fb8c3","trusted":true}},{"cell_type":"code","source":"def get_dataloaders(annotations, config): \n    train_set_temp, test_annotations = train_test_split(annotations, test_size=0.2, random_state=random_state, stratify=annotations['CLASS-ID'])\n    train_annotations, validation_annotations = train_test_split(train_set_temp, test_size=0.2, random_state=random_state, stratify=train_set_temp['CLASS-ID'])\n\n    train_data = CatsDogsDataset(train_annotations, transform=config.train_transforms)\n    valid_data = CatsDogsDataset(validation_annotations, transform=config.test_transforms)\n    test_data = CatsDogsDataset(test_annotations, transform=config.test_transforms)\n\n    train_loader = DataLoader(dataset=train_data, batch_size=config.batch_size , shuffle=True)\n    valid_loader = DataLoader(dataset=valid_data, batch_size=config.batch_size, shuffle=True)\n    test_loader = DataLoader(dataset=test_data, batch_size=config.batch_size, shuffle=False)\n    \n    return train_loader,  valid_loader, test_loader","metadata":{"_uuid":"7b09690e-55d9-45a4-a758-cd93632a811b","_cell_guid":"3d204d7d-4140-4554-8394-da7d27835371","execution":{"iopub.status.busy":"2024-02-08T14:20:41.107306Z","iopub.execute_input":"2024-02-08T14:20:41.107560Z","iopub.status.idle":"2024-02-08T14:20:41.117662Z","shell.execute_reply.started":"2024-02-08T14:20:41.107539Z","shell.execute_reply":"2024-02-08T14:20:41.116841Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"_uuid":"2e2488d5-d2a4-4c4c-bfee-70c6b10acb68","_cell_guid":"ffea4198-5702-4889-bc8b-6e84246b230d","trusted":true}},{"cell_type":"markdown","source":"### Accuracy function","metadata":{"_uuid":"2bed54b5-5fff-40c6-8ca2-b2a4594349cb","_cell_guid":"9409ae66-18bc-4297-9278-bb4081319ef6","trusted":true}},{"cell_type":"code","source":"def my_accuracy(predictions, labels):\n    predictions = torch.argmax(predictions,dim=1)\n    #print(predictions)\n    #print(labels)\n    correct = (predictions == labels)\n    #print(correct)\n    acc = sum(correct) / len(predictions)\n    #print(acc.item())\n    return acc.item()","metadata":{"_uuid":"c6faa626-6dda-4d5e-8c39-a63c93340f40","_cell_guid":"ff039d46-8087-4803-964b-c2f5c8a8f8c9","execution":{"iopub.status.busy":"2024-02-08T14:20:41.118693Z","iopub.execute_input":"2024-02-08T14:20:41.118939Z","iopub.status.idle":"2024-02-08T14:20:41.130732Z","shell.execute_reply.started":"2024-02-08T14:20:41.118918Z","shell.execute_reply":"2024-02-08T14:20:41.129979Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Saving checkpoint","metadata":{"_uuid":"a4637c80-ac5c-489e-b4a1-156631578603","_cell_guid":"b97b9e8a-1172-4bdb-b5f3-0f9d3a56ceb4","trusted":true}},{"cell_type":"code","source":"def save_checkpoint(epoch, model, optimizer, loss, config):\n    if epoch%3 == 0:\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(prefix='modified_fc.'),\n            'optimizer_state_dict': optimizer.state_dict(), # contains information like lr scheduler state\n            'loss': loss,  # Save the current training loss if needed\n        }\n        info = config.get_configuration_info()\n        checkpoint_path = f'/kaggle/working/{epoch}_{info}.pth'\n        torch.save(checkpoint, checkpoint_path)\n        config.last_checkpoint_path = checkpoint_path\n    return","metadata":{"_uuid":"b80a8abc-5349-47d7-83bb-a0c5a1b1120c","_cell_guid":"63936c3b-4f4c-43d0-af07-8faaf7c8d58c","execution":{"iopub.status.busy":"2024-02-08T14:20:41.131671Z","iopub.execute_input":"2024-02-08T14:20:41.131913Z","iopub.status.idle":"2024-02-08T14:20:41.141069Z","shell.execute_reply.started":"2024-02-08T14:20:41.131892Z","shell.execute_reply":"2024-02-08T14:20:41.140265Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Training loop","metadata":{"_uuid":"967732e8-2b82-4cbc-91b4-b70863ddbf08","_cell_guid":"6cc568ce-e3b4-4c36-a4bc-aeb7625ec7d4","trusted":true}},{"cell_type":"code","source":"class TrainingManager: # singleton class\n    _instance = None  # Class variable to store the instance\n\n    def __new__(cls, *args, **kwargs):\n        if not cls._instance:\n            cls._instance = super(TrainingManager, cls).__new__(cls, *args, **kwargs)\n        return cls._instance\n    \n    def __init__(self):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.net = None\n        self.optimizer = None\n        self.scheduler = None\n        self.criterion = None\n        \n        self.train_loader = None\n        self.valid_loader = None\n        self.test_loader = None\n        self.resume_epoch = None\n        self.training_accuracy_epoch = 0\n        self.validation_accuracy_epoch = 0\n        self.training_loss_epoch = 0\n        self.validation_loss_epoch = 100\n        self.total_train_batches = 0\n        self.total_valid_batches = 0\n        self.resume_epoch = 0\n        self.cnt = 0\n        \n        self.log_interval = 1\n        self.mean_loss = 0.0\n        self.mean_lr = 0.0\n        self.mean_grad_magnitude = 0.0\n        self.termination_counter = 0\n        self.last_validation_loss = 0\n\n    def setup_configuration(self, config, annotations):\n        self.resume_epoch = 0\n        self.net = config.model.to(self.device)\n        self.optimizer = config.optimizer\n        self.scheduler = config.scheduler\n        self.criterion = config.criterion\n        self.train_loader, self.valid_loader, self.test_loader = get_dataloaders(annotations, config)\n        \n        self.total_train_batches = len(self.train_loader)\n        self.total_valid_batches = len(self.valid_loader)\n        \n        if config.last_checkpoint_path != None:\n            checkpoint = torch.load(config.last_checkpoint_path)\n            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            self.resume_epoch = checkpoint['epoch']\n            # Remove the prefix from the keys\n            state_dict = {k.replace('modified_fc.', ''): v for k, v in checkpoint['model_state_dict'].items()}\n            # It is required after DataParallel wraps the module\n            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n            self.net.load_state_dict(state_dict)\n            \n        d = next(self.net.parameters()).device\n        if d.type == 'cuda':\n            print(\"Model is on GPU\")\n            self.net = torch.nn.DataParallel(self.net) # if multiple GPUs use them\n        else:\n            print(\"Model is on CPU\")\n                \n    def training(self, config, annotations):\n        num_epochs = 25\n        self.termination_counter = 0\n        self.setup_configuration(config, annotations)\n        self.net.train()\n        for epoch in range(num_epochs - self.resume_epoch):\n            self.training_loss_epoch = 0\n            self.training_accuracy_epoch = 0\n            self.cnt = 0\n\n            for train_features_batch, train_labels_batch in self.train_loader:\n                train_features_batch, train_labels_batch = train_features_batch.to(self.device), train_labels_batch.to(self.device)\n                self.optimizer.zero_grad()\n\n                y_hat = self.net(train_features_batch)\n                \n                #print(train_labels_batch)\n                #print(y_hat)\n\n                training_loss = self.criterion(y_hat, train_labels_batch)\n                training_loss.backward()\n                self.optimizer.step()\n\n                self.training_loss_epoch += training_loss.item()\n                train_accuracy = my_accuracy(y_hat, train_labels_batch)\n                self.training_accuracy_epoch += train_accuracy\n                \n                self.cnt += 1\n                self.batch_log_metrics(training_loss)\n                \n            save_checkpoint(epoch+self.resume_epoch, self.net, self.optimizer, self.training_loss_epoch, config)\n            self.last_validation_loss = self.validation_loss_epoch\n            self.validation_loss_epoch = 0\n            self.validation_accuracy_epoch = 0\n            self.cnt = 0\n\n            for val_features_batch, val_labels_batch in self.valid_loader:\n                val_features_batch, val_labels_batch = val_features_batch.to(self.device), val_labels_batch.to(self.device)\n                with torch.no_grad():\n                    y_hat_val = self.net(val_features_batch)\n                    validation_loss = self.criterion(y_hat_val, val_labels_batch)\n                    self.validation_loss_epoch += validation_loss.item()\n                    val_accuracy = my_accuracy(y_hat_val, val_labels_batch)\n                    self.validation_accuracy_epoch += val_accuracy\n                    self.cnt += 1\n                    \n            self.normalize_metrics()      \n            self.log_metrics()\n            self.print_metrics(epoch + self.resume_epoch)\n            self.scheduler.step(self.validation_loss_epoch)\n            # Early stopping\n            if  (self.validation_accuracy_epoch - self.training_accuracy_epoch < -0.20) and (self.training_loss_epoch - self.validation_loss_epoch < -0.15):\n                save_checkpoint(epoch+self.resume_epoch, self.net, self.optimizer, self.training_loss_epoch, config)\n                print('Early stopping! Overfitting')\n                return\n            if  self.validation_loss_epoch >= self.last_validation_loss:\n                self.termination_counter += 1\n                save_checkpoint(epoch+self.resume_epoch, self.net, self.optimizer, self.training_loss_epoch, config)\n                if self.termination_counter > 3:\n                    print('Early stopping! Overfitting')\n                    return\n            else:\n                self.termination_counter = 0\n        save_checkpoint(epoch+self.resume_epoch, self.net, self.optimizer, self.training_loss_epoch, config)\n        return\n    def normalize_metrics(self):\n            self.training_accuracy_epoch = self.training_accuracy_epoch / self.total_train_batches\n            self.validation_accuracy_epoch = self.validation_accuracy_epoch / self.total_valid_batches\n            self.training_loss_epoch = self.training_loss_epoch / self.total_train_batches\n            self.validation_loss_epoch = self.validation_loss_epoch / self.total_valid_batches\n            \n    def log_metrics(self):\n        wandb.log({f'validation_loss_epoch': self.validation_loss_epoch,\n                   f'training_loss_epoch': self.training_loss_epoch})\n        wandb.log({f'training_accuracy_epoch': self.training_accuracy_epoch,\n                   f'validation_accuracy_epoch': self.validation_accuracy_epoch})\n\n    def print_metrics(self, epoch):\n        print('~~~~~~~~~~~~~~~~~~~~~ Epoch: ', epoch, ' ~~~~~~~~~~~~~~~~~~~~~')\n        print({f'training_loss_epoch': self.training_loss_epoch,\n               f'training_accuracy_epoch': self.training_accuracy_epoch})\n        print({f'validation_loss_epoch': self.validation_loss_epoch,\n               f'validation_accuracy_epoch': self.validation_accuracy_epoch})\n        \n    def batch_log_metrics(self, train_loss):\n        # Calculate mean values every n batches\n        self.mean_loss += train_loss.item()\n        self.mean_lr += self.optimizer.param_groups[0]['lr']\n        self.mean_grad_magnitude += self.calculate_gradient_magnitude()\n\n        if self.cnt % self.log_interval == 0:\n            # Log mean values\n            mean_loss_batch = self.mean_loss / self.log_interval\n            mean_lr_batch = self.mean_lr / self.log_interval\n            mean_grad_magnitude_batch = self.mean_grad_magnitude / self.log_interval\n\n            wandb.log({'training_loss_every_n_batches': mean_loss_batch,\n                       'training_learning_rate_every_n_batches': mean_lr_batch,\n                       'training_gradient_magnitude_every_n_batches': mean_grad_magnitude_batch})\n\n            # Reset mean values\n            self.mean_loss = 0.0\n            self.mean_lr = 0.0\n            self.mean_grad_magnitude = 0.0\n            \n    def calculate_gradient_magnitude(self):\n        # Example implementation of calculating the gradient magnitude\n        total_norm = 0.0\n        for param in self.net.parameters():\n            if param.grad is not None:\n                total_norm += param.grad.data.norm(2).item()\n        return total_norm\n    def evaluate_on_test_dataset(config, annotations):\n        #setup_configuration(self, config, annotations)\n        #self.net.eval()\n        config.model.eval()\n        self.train_loader, self.valid_loader, self.test_loader = get_dataloaders(annotations, config)\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for inputs_batch, labels_batch in self.test_loader:\n                outputs = model(inputs_batch)\n                predictions.append(outputs)\n                labels.append(labels_batch)\n        print(my_accuracy(predictions,labels))\n        \n    def evaluate_on_test_dataset(self, config, annotations):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.setup_configuration(config, annotations)\n        self.net.eval()\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for inputs_batch, labels_batch in self.test_loader:\n                inputs_batch = inputs_batch.to(self.device)\n                labels_batch = labels_batch.tolist()\n                outputs = self.net(inputs_batch)\n                predicted_classes = [torch.argmax(pred).item() for pred in outputs]\n                predictions += predicted_classes\n                labels += labels_batch\n        return accuracy_score(labels, predictions), confusion_matrix(labels, predictions)","metadata":{"_uuid":"420cc670-9d12-4fdf-baf3-a1fc4ba8ce7e","_cell_guid":"75fd7445-fb6e-4112-885c-19f166ad754c","execution":{"iopub.status.busy":"2024-02-08T14:20:41.142168Z","iopub.execute_input":"2024-02-08T14:20:41.142432Z","iopub.status.idle":"2024-02-08T14:20:41.179490Z","shell.execute_reply.started":"2024-02-08T14:20:41.142403Z","shell.execute_reply":"2024-02-08T14:20:41.178584Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Defining models","metadata":{"_uuid":"63e21653-39e1-42f2-9c24-d447dfd03a61","_cell_guid":"6c40ad0a-e194-4d86-a354-d34ddacf9809","trusted":true}},{"cell_type":"markdown","source":"### ResNet101","metadata":{"_uuid":"95c2a756-ddf1-4cfc-a9a7-a1fb9869d7a7","_cell_guid":"e515a9ae-44ec-4e14-b71a-e7fa4e9529c1","trusted":true}},{"cell_type":"code","source":"def get_pretrained_resnet101():\n    model = models.resnet101(weights=torchvision.models.ResNet101_Weights.DEFAULT)\n    num_classes = 37\n    model.fc = nn.Sequential(\n        nn.Linear(model.fc.in_features, num_classes),\n        nn.Softmax(dim=1)\n    )\n    nn.init.xavier_normal_(model.fc[0].weight)\n    nn.init.constant_(model.fc[0].bias, 0)\n    \n    weights = torchvision.models.ResNet101_Weights.DEFAULT\n    preprocess_transforms = weights.transforms()\n    \n    return model, preprocess_transforms","metadata":{"_uuid":"adcfbbc2-844a-4ae1-b6f1-0cf364f48dbc","_cell_guid":"81d61e69-651c-40f6-9241-dc38acb40934","execution":{"iopub.status.busy":"2024-02-08T14:20:41.180445Z","iopub.execute_input":"2024-02-08T14:20:41.180741Z","iopub.status.idle":"2024-02-08T14:20:41.194825Z","shell.execute_reply.started":"2024-02-08T14:20:41.180718Z","shell.execute_reply":"2024-02-08T14:20:41.193890Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Inception V3","metadata":{}},{"cell_type":"code","source":"def remove_auxiliary_logits(model):\n    if isinstance(model, models.Inception3):\n        # Set the auxiliary logits to False\n        model.aux_logits = False\n        \n        # Replace the auxiliary classifier with an identity layer\n        model.AuxLogits = nn.Identity()\n\ndef get_pretrained_inception_v3():\n    model = torchvision.models.inception_v3(pretrained=True)\n    \n    remove_auxiliary_logits(model)\n    \n    # Replace the final fully connected layer with a new one\n    num_classes = 37\n    model.fc = nn.Sequential(\n        nn.Linear(model.fc.in_features, num_classes),\n        nn.Softmax(dim=1)\n    )\n    \n    # Initialize the new fully connected layer\n    nn.init.xavier_normal_(model.fc[0].weight)\n    nn.init.constant_(model.fc[0].bias, 0)\n    \n    # Get the default preprocessing transforms for Inception V3\n    preprocess_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize((299,299)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n    ])\n    \n    return model, preprocess_transforms","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:20:41.196081Z","iopub.execute_input":"2024-02-08T14:20:41.196438Z","iopub.status.idle":"2024-02-08T14:20:41.206975Z","shell.execute_reply.started":"2024-02-08T14:20:41.196406Z","shell.execute_reply":"2024-02-08T14:20:41.206089Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"passed_config = {\n        'optimizer': 'sgd',\n        'learning_rate' : 0.02,\n        'batch_size': 64,\n    }","metadata":{"_uuid":"8b0d9253-f9d3-46a8-bdf7-f7c1bba9e7c7","_cell_guid":"3a882d0b-7f35-4a30-ac61-14d799aee814","execution":{"iopub.status.busy":"2024-02-08T14:20:41.208253Z","iopub.execute_input":"2024-02-08T14:20:41.208541Z","iopub.status.idle":"2024-02-08T14:20:41.220971Z","shell.execute_reply.started":"2024-02-08T14:20:41.208518Z","shell.execute_reply":"2024-02-08T14:20:41.220223Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',\n    'name': 'final_tuning_ResNet101',\n    \"metric\": {\"goal\": \"minimize\", \"name\": \"validation_loss_epoch\"},\n    'parameters': {\n        'model': {'values': ['ResNet101']},\n        'optimizer': {'values': ['sgd', 'adam']},\n        'learning_rate': {'min': 0.0001, 'max': 0.07},\n        'batch_size': {'values': [32, 64]},\n    }\n}","metadata":{"_uuid":"6b812fb1-bee5-48c0-8c6e-16e313ec8c64","_cell_guid":"efa3a7b3-9752-444d-bd1c-c0a9da65a910","execution":{"iopub.status.busy":"2024-02-08T14:20:41.222180Z","iopub.execute_input":"2024-02-08T14:20:41.222786Z","iopub.status.idle":"2024-02-08T14:20:41.231584Z","shell.execute_reply.started":"2024-02-08T14:20:41.222745Z","shell.execute_reply":"2024-02-08T14:20:41.230803Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#initialize_wandb(sweep_config)\n#print(wandb.config)","metadata":{"_uuid":"13fbeb00-c025-4571-9697-a1e47517055c","_cell_guid":"332ac1e1-d0c8-439c-9feb-25f080ad8332","execution":{"iopub.status.busy":"2024-02-08T14:20:41.232719Z","iopub.execute_input":"2024-02-08T14:20:41.233246Z","iopub.status.idle":"2024-02-08T14:20:41.242828Z","shell.execute_reply.started":"2024-02-08T14:20:41.233215Z","shell.execute_reply":"2024-02-08T14:20:41.241998Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train_model(config):\n    model, preprocess_transforms = get_pretrained_inception_v3()\n    traning_config = configuration(config, model, preprocess_transforms)\n    manager = TrainingManager()\n    manager.training(traning_config, annotations)","metadata":{"_uuid":"74d527f8-6efe-4090-9a1d-54280263d7af","_cell_guid":"6f01a231-f65c-4774-a861-4903d57ced0b","execution":{"iopub.status.busy":"2024-02-08T14:20:41.243995Z","iopub.execute_input":"2024-02-08T14:20:41.244892Z","iopub.status.idle":"2024-02-08T14:20:41.254387Z","shell.execute_reply.started":"2024-02-08T14:20:41.244859Z","shell.execute_reply":"2024-02-08T14:20:41.253466Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def start_training_with_sweep():\n    wandb.init()\n    print(wandb.config)\n    print(\"*\")\n    model, preprocess_transforms = get_pretrained_inception_v3()\n    traning_config = configuration(wandb.config, model, preprocess_transforms)\n    manager = TrainingManager()\n    manager.training(traning_config, annotations)","metadata":{"_uuid":"7bd1f441-f280-403d-a46c-31cfe9985e8d","_cell_guid":"d1b937ea-843a-43f2-ab3b-71baaa83734b","execution":{"iopub.status.busy":"2024-02-08T14:20:41.255566Z","iopub.execute_input":"2024-02-08T14:20:41.255873Z","iopub.status.idle":"2024-02-08T14:20:41.264671Z","shell.execute_reply.started":"2024-02-08T14:20:41.255848Z","shell.execute_reply":"2024-02-08T14:20:41.263859Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(passed_config)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T14:20:41.265684Z","iopub.execute_input":"2024-02-08T14:20:41.265953Z","iopub.status.idle":"2024-02-08T14:20:41.285011Z","shell.execute_reply.started":"2024-02-08T14:20:41.265931Z","shell.execute_reply":"2024-02-08T14:20:41.284016Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"{'optimizer': 'sgd', 'learning_rate': 0.02, 'batch_size': 64}\n","output_type":"stream"}]},{"cell_type":"code","source":"initialize_wandb_with_config(passed_config)\nprint(passed_config)\nprint('config')\nprint(wandb.config)\ntrain_model(passed_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initialize_wandb()\nsweep_id = wandb.sweep(sweep=sweep_config, project=\"cats&dogs_ML&DL_project\")\nprint(wandb.config)\nwandb.agent(sweep_id, function=start_training_with_sweep, count=15)","metadata":{"_uuid":"db4dba4f-326b-41b5-8732-b8199756c4b5","_cell_guid":"a3848d77-c012-46cf-9799-cb80a56d8dd2","execution":{"iopub.status.busy":"2024-02-08T14:20:44.902461Z","iopub.execute_input":"2024-02-08T14:20:44.903352Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjerzyjerzu\u001b[0m (\u001b[33mjerze\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240208_142047-3k03hgea</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jerze/uncategorized/runs/3k03hgea' target=\"_blank\">dainty-butterfly-126</a></strong> to <a href='https://wandb.ai/jerze/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jerze/uncategorized' target=\"_blank\">https://wandb.ai/jerze/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jerze/uncategorized/runs/3k03hgea' target=\"_blank\">https://wandb.ai/jerze/uncategorized/runs/3k03hgea</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: 01tf0bop\nSweep URL: https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop\n{}\nVBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))\n<IPython.core.display.HTML object>\n<IPython.core.display.HTML object>\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jqcaoc6g with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.03318406299873464\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ResNet101\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240208_142145-jqcaoc6g</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/jqcaoc6g' target=\"_blank\">earnest-sweep-1</a></strong> to <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/jqcaoc6g' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/jqcaoc6g</a>"},"metadata":{}},{"name":"stdout","text":"{'batch_size': 64, 'learning_rate': 0.03318406299873464, 'model': 'ResNet101', 'optimizer': 'adam'}\n*\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 209MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"configuraiton class  {'batch_size': 64, 'learning_rate': 0.03318406299873464, 'model': 'ResNet101', 'optimizer': 'adam'}\nloading parameters\nbatch\nlr\noptimizer\nloaded parameters\ngetting optimizer\nadam\ngot optimizer\nModel is on GPU\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  0  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6147033556087598, 'training_accuracy_epoch': 0.026393581081081082}\n{'validation_loss_epoch': 3.6099234631187036, 'validation_accuracy_epoch': 0.03179824567939106}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  1  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.613116032368428, 'training_accuracy_epoch': 0.024704391891891893}\n{'validation_loss_epoch': 3.6148885300284936, 'validation_accuracy_epoch': 0.018640350942548952}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  2  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.612783309575674, 'training_accuracy_epoch': 0.02387341977776708}\n{'validation_loss_epoch': 3.6109086965259753, 'validation_accuracy_epoch': 0.032072368421052634}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  3  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.612125602928368, 'training_accuracy_epoch': 0.022804054054054054}\n{'validation_loss_epoch': 3.610868479076185, 'validation_accuracy_epoch': 0.02686403515307527}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  4  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6139039961067407, 'training_accuracy_epoch': 0.024929163021010323}\n{'validation_loss_epoch': 3.6103349108445015, 'validation_accuracy_epoch': 0.03344298252149632}\nEpoch 00005: reducing learning rate of group 0 to 1.6592e-02.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  5  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6121698489060274, 'training_accuracy_epoch': 0.029376907123101724}\n{'validation_loss_epoch': 3.6114868239352576, 'validation_accuracy_epoch': 0.024396929889917374}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  6  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6122888971019433, 'training_accuracy_epoch': 0.027265420636615238}\n{'validation_loss_epoch': 3.6106732268082466, 'validation_accuracy_epoch': 0.023574561468864743}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  7  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611313339826223, 'training_accuracy_epoch': 0.024506865723713023}\n{'validation_loss_epoch': 3.611600248437179, 'validation_accuracy_epoch': 0.02549342105263158}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  8  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6116622009792843, 'training_accuracy_epoch': 0.030405405405405407}\n{'validation_loss_epoch': 3.615363735901682, 'validation_accuracy_epoch': 0.022203947368421052}\nEpoch 00009: reducing learning rate of group 0 to 8.2960e-03.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  9  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6132021046973564, 'training_accuracy_epoch': 0.02660472972972973}\n{'validation_loss_epoch': 3.612700663114849, 'validation_accuracy_epoch': 0.02549342105263158}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  10  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6130519525424853, 'training_accuracy_epoch': 0.025773757615604916}\n{'validation_loss_epoch': 3.610772208163613, 'validation_accuracy_epoch': 0.03591008778465422}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  11  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6129476895203463, 'training_accuracy_epoch': 0.025984906264253566}\n{'validation_loss_epoch': 3.6093237023604545, 'validation_accuracy_epoch': 0.030153508837285795}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  12  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611904124955873, 'training_accuracy_epoch': 0.0283075413993887}\n{'validation_loss_epoch': 3.6116013652399968, 'validation_accuracy_epoch': 0.03125}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  13  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.609209189543853, 'training_accuracy_epoch': 0.034035799873841775}\n{'validation_loss_epoch': 3.6073645917992843, 'validation_accuracy_epoch': 0.032072368421052634}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  14  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6119227344925338, 'training_accuracy_epoch': 0.02514031166965897}\n{'validation_loss_epoch': 3.6132199387801323, 'validation_accuracy_epoch': 0.027960526315789474}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  15  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.609491335379111, 'training_accuracy_epoch': 0.033586257615604916}\n{'validation_loss_epoch': 3.603188891159861, 'validation_accuracy_epoch': 0.03563596504299264}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  16  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6049846056345345, 'training_accuracy_epoch': 0.0405677855014801}\n{'validation_loss_epoch': 3.6060708949440405, 'validation_accuracy_epoch': 0.036732456205706844}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  17  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.604205994992643, 'training_accuracy_epoch': 0.03908974496093956}\n{'validation_loss_epoch': 3.6020102877365914, 'validation_accuracy_epoch': 0.04248903515307527}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  18  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6044792613467656, 'training_accuracy_epoch': 0.03886497383182113}\n{'validation_loss_epoch': 3.608636165920057, 'validation_accuracy_epoch': 0.03645833346404528}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  19  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6078112383146546, 'training_accuracy_epoch': 0.03462837837837838}\n{'validation_loss_epoch': 3.601826216045179, 'validation_accuracy_epoch': 0.0397478071482558}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  20  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6012321613930367, 'training_accuracy_epoch': 0.045213055771750374}\n{'validation_loss_epoch': 3.596775920767533, 'validation_accuracy_epoch': 0.04879385977983475}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  21  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6004686806653, 'training_accuracy_epoch': 0.045199433291280594}\n{'validation_loss_epoch': 3.5993569399181164, 'validation_accuracy_epoch': 0.04413377199518053}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  22  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.599018886282637, 'training_accuracy_epoch': 0.041821054912902215}\n{'validation_loss_epoch': 3.5996197650307105, 'validation_accuracy_epoch': 0.04358552631578947}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  23  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.5992821132814563, 'training_accuracy_epoch': 0.04308794680479411}\n{'validation_loss_epoch': 3.5989464709633276, 'validation_accuracy_epoch': 0.046600877258338426}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  24  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.599341379629599, 'training_accuracy_epoch': 0.04604402788587519}\n{'validation_loss_epoch': 3.6021666275827506, 'validation_accuracy_epoch': 0.04194078947368421}\nEpoch 00025: reducing learning rate of group 0 to 4.1480e-03.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>▂▂▁▁▂▃▂▂▃▂▂▂▃▄▂▄▆▆▆▅██▇▇█</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>▂▂▂▂▂▁▁▂▂▁▁▁▁▂█▂▂▄▁▂▂▃▂▂▃▄▂▃▂▄▂▂▂▄▂▂▃▂▃▄</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>████████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss_epoch</td><td>█▇▇▇█▇▇▆▇▇▇▇▇▆▇▆▄▃▃▅▂▂▁▁▁</td></tr><tr><td>training_loss_every_n_batches</td><td>▆▇█▅▇▆▆▆▆▆▆▆▆▆▆▆▄▂▇▆▄▄▆▅▆▅▅▃▃▂▆▂▁▅▂▆▃▅▂▄</td></tr><tr><td>validation_accuracy_epoch</td><td>▄▁▄▃▄▂▂▃▂▃▅▄▄▄▃▅▅▇▅▆█▇▇▇▆</td></tr><tr><td>validation_loss_epoch</td><td>▆█▆▆▆▇▆▇█▇▆▆▇▅▇▃▅▃▅▃▁▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>0.04604</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>0.11652</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>0.0083</td></tr><tr><td>training_loss_epoch</td><td>3.59934</td></tr><tr><td>training_loss_every_n_batches</td><td>3.61194</td></tr><tr><td>validation_accuracy_epoch</td><td>0.04194</td></tr><tr><td>validation_loss_epoch</td><td>3.60217</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">earnest-sweep-1</strong> at: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/jqcaoc6g' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/jqcaoc6g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240208_142145-jqcaoc6g/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xly5z052 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.06873333402796476\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ResNet101\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240208_151724-xly5z052</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/xly5z052' target=\"_blank\">solar-sweep-2</a></strong> to <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/xly5z052' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/xly5z052</a>"},"metadata":{}},{"name":"stdout","text":"{'batch_size': 32, 'learning_rate': 0.06873333402796476, 'model': 'ResNet101', 'optimizer': 'adam'}\n*\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"configuraiton class  {'batch_size': 32, 'learning_rate': 0.06873333402796476, 'model': 'ResNet101', 'optimizer': 'adam'}\nloading parameters\nbatch\nlr\noptimizer\nloaded parameters\ngetting optimizer\nadam\ngot optimizer\nModel is on GPU\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  0  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6286886662853006, 'training_accuracy_epoch': 0.026579986826903153}\n{'validation_loss_epoch': 3.6306915992015116, 'validation_accuracy_epoch': 0.025619369402930543}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  1  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.630842601360918, 'training_accuracy_epoch': 0.025510204081632654}\n{'validation_loss_epoch': 3.629284246547802, 'validation_accuracy_epoch': 0.02702702702702703}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  2  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.629312974255101, 'training_accuracy_epoch': 0.02699829931972789}\n{'validation_loss_epoch': 3.629002719312101, 'validation_accuracy_epoch': 0.027308558592119732}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  3  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6293061168826357, 'training_accuracy_epoch': 0.027005156894930365}\n{'validation_loss_epoch': 3.629002719312101, 'validation_accuracy_epoch': 0.027308558592119732}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  4  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.629312974255101, 'training_accuracy_epoch': 0.02699829931972789}\n{'validation_loss_epoch': 3.629284246547802, 'validation_accuracy_epoch': 0.02702702702702703}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  5  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.62929925951017, 'training_accuracy_epoch': 0.02701201447013284}\n{'validation_loss_epoch': 3.629002719312101, 'validation_accuracy_epoch': 0.027308558592119732}\nEpoch 00006: reducing learning rate of group 0 to 3.4367e-02.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  6  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.62929925951017, 'training_accuracy_epoch': 0.02701201447013284}\n{'validation_loss_epoch': 3.629284246547802, 'validation_accuracy_epoch': 0.02702702702702703}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  7  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6293061168826357, 'training_accuracy_epoch': 0.027005156894930365}\n{'validation_loss_epoch': 3.629284246547802, 'validation_accuracy_epoch': 0.02702702702702703}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  8  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.629312974255101, 'training_accuracy_epoch': 0.02699829931972789}\n{'validation_loss_epoch': 3.629284246547802, 'validation_accuracy_epoch': 0.02702702702702703}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  9  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6293061168826357, 'training_accuracy_epoch': 0.027005156894930365}\n{'validation_loss_epoch': 3.629002719312101, 'validation_accuracy_epoch': 0.027308558592119732}\nEpoch 00010: reducing learning rate of group 0 to 1.7183e-02.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  10  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.629312974255101, 'training_accuracy_epoch': 0.02699829931972789}\n{'validation_loss_epoch': 3.629284246547802, 'validation_accuracy_epoch': 0.02702702702702703}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  11  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.629312974255101, 'training_accuracy_epoch': 0.02699829931972789}\n{'validation_loss_epoch': 3.629284246547802, 'validation_accuracy_epoch': 0.02702702702702703}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  12  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.629312974255101, 'training_accuracy_epoch': 0.02699829931972789}\n{'validation_loss_epoch': 3.629284246547802, 'validation_accuracy_epoch': 0.02702702702702703}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  13  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6293061168826357, 'training_accuracy_epoch': 0.027005156894930365}\n{'validation_loss_epoch': 3.629002719312101, 'validation_accuracy_epoch': 0.027308558592119732}\nEpoch 00014: reducing learning rate of group 0 to 8.5917e-03.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  14  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6293061168826357, 'training_accuracy_epoch': 0.027005156894930365}\n{'validation_loss_epoch': 3.629002719312101, 'validation_accuracy_epoch': 0.027308558592119732}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  15  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.62929925951017, 'training_accuracy_epoch': 0.02701201447013284}\n{'validation_loss_epoch': 3.629002719312101, 'validation_accuracy_epoch': 0.027308558592119732}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  16  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.629312974255101, 'training_accuracy_epoch': 0.02699829931972789}\n{'validation_loss_epoch': 3.629002719312101, 'validation_accuracy_epoch': 0.027308558592119732}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  17  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.629312974255101, 'training_accuracy_epoch': 0.02699829931972789}\n{'validation_loss_epoch': 3.629284246547802, 'validation_accuracy_epoch': 0.02702702702702703}\nEpoch 00018: reducing learning rate of group 0 to 4.2958e-03.\nEarly stopping! Overfitting\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>▆▁████████████████</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>█████████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss_epoch</td><td>▁█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>training_loss_every_n_batches</td><td>▃█▃█▁▆▃▃▆█▆██▆▆█▆█▃█▆█████▆█▃▆█▆▃▆▃█▃▆█▃</td></tr><tr><td>validation_accuracy_epoch</td><td>▁▇██▇█▇▇▇█▇▇▇████▇</td></tr><tr><td>validation_loss_epoch</td><td>█▂▁▁▂▁▂▂▂▁▂▂▂▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>0.027</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>0.0</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>0.00859</td></tr><tr><td>training_loss_epoch</td><td>3.62931</td></tr><tr><td>training_loss_every_n_batches</td><td>3.65631</td></tr><tr><td>validation_accuracy_epoch</td><td>0.02703</td></tr><tr><td>validation_loss_epoch</td><td>3.62928</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">solar-sweep-2</strong> at: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/xly5z052' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/xly5z052</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240208_151724-xly5z052/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9wxm37b8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0103862002430907\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ResNet101\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240208_155720-9wxm37b8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/9wxm37b8' target=\"_blank\">faithful-sweep-3</a></strong> to <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/9wxm37b8' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/9wxm37b8</a>"},"metadata":{}},{"name":"stdout","text":"{'batch_size': 32, 'learning_rate': 0.0103862002430907, 'model': 'ResNet101', 'optimizer': 'sgd'}\n*\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"configuraiton class  {'batch_size': 32, 'learning_rate': 0.0103862002430907, 'model': 'ResNet101', 'optimizer': 'sgd'}\nloading parameters\nbatch\nlr\noptimizer\nloaded parameters\ngetting optimizer\nsgd\ngot optimizer\nModel is on GPU\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  0  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.505041370586473, 'training_accuracy_epoch': 0.1880143185456594}\n{'validation_loss_epoch': 3.204584534103806, 'validation_accuracy_epoch': 0.5129504509874292}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  1  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.168060764974477, 'training_accuracy_epoch': 0.5317917486437324}\n{'validation_loss_epoch': 2.999714890041867, 'validation_accuracy_epoch': 0.6722972972972973}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  3  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.064048551377796, 'training_accuracy_epoch': 0.6096253013935219}\n{'validation_loss_epoch': 2.933294360702102, 'validation_accuracy_epoch': 0.7401463969333751}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  4  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.039747022447132, 'training_accuracy_epoch': 0.6300128921359575}\n{'validation_loss_epoch': 2.9324041121714823, 'validation_accuracy_epoch': 0.7347972972972973}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  5  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.0328949146530255, 'training_accuracy_epoch': 0.6357526880543248}\n{'validation_loss_epoch': 2.91467174968204, 'validation_accuracy_epoch': 0.7514076587316152}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  6  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.0286163862059716, 'training_accuracy_epoch': 0.6406627163595083}\n{'validation_loss_epoch': 2.9262668377644308, 'validation_accuracy_epoch': 0.7409909915279698}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  7  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.014810943279137, 'training_accuracy_epoch': 0.653616688689407}\n{'validation_loss_epoch': 2.906101201031659, 'validation_accuracy_epoch': 0.7587274780144563}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  8  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.004491679522456, 'training_accuracy_epoch': 0.6610845951806932}\n{'validation_loss_epoch': 2.9013131760262154, 'validation_accuracy_epoch': 0.7618243243243243}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  9  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9964904379682475, 'training_accuracy_epoch': 0.6674690035735669}\n{'validation_loss_epoch': 2.895710307198602, 'validation_accuracy_epoch': 0.7708333344072908}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  10  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9894356451877933, 'training_accuracy_epoch': 0.6757461047496925}\n{'validation_loss_epoch': 2.9176259105269975, 'validation_accuracy_epoch': 0.7409909915279698}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  11  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.996466589622757, 'training_accuracy_epoch': 0.6670095458322641}\n{'validation_loss_epoch': 2.9035843642982275, 'validation_accuracy_epoch': 0.7595720726090509}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  12  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.976467633733944, 'training_accuracy_epoch': 0.6870268267028186}\n{'validation_loss_epoch': 2.9013894635277824, 'validation_accuracy_epoch': 0.7595720726090509}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  13  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9802660017597433, 'training_accuracy_epoch': 0.683392308196243}\n{'validation_loss_epoch': 2.906376484278086, 'validation_accuracy_epoch': 0.7573198208937774}\nEpoch 00014: reducing learning rate of group 0 to 5.1931e-03.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  14  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.957355325724803, 'training_accuracy_epoch': 0.704239357085455}\n{'validation_loss_epoch': 2.878850511602453, 'validation_accuracy_epoch': 0.7789977482847266}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  15  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9443202164708353, 'training_accuracy_epoch': 0.7178447992623258}\n{'validation_loss_epoch': 2.8804047623196163, 'validation_accuracy_epoch': 0.7795608108108109}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  16  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9440698510124568, 'training_accuracy_epoch': 0.7169944591262714}\n{'validation_loss_epoch': 2.873271523295222, 'validation_accuracy_epoch': 0.7868806317045882}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  17  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9379471094429896, 'training_accuracy_epoch': 0.722939982300713}\n{'validation_loss_epoch': 2.8765802770047575, 'validation_accuracy_epoch': 0.7820945945945946}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  18  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.928246796536608, 'training_accuracy_epoch': 0.732081138763298}\n{'validation_loss_epoch': 2.868970980515351, 'validation_accuracy_epoch': 0.7911036046775611}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  19  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9242721820364195, 'training_accuracy_epoch': 0.7346527316943318}\n{'validation_loss_epoch': 2.8693418760557434, 'validation_accuracy_epoch': 0.7880067567567568}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  20  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.916711393667727, 'training_accuracy_epoch': 0.7454877110565601}\n{'validation_loss_epoch': 2.86904095959019, 'validation_accuracy_epoch': 0.7896959459459459}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  21  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9222913631776564, 'training_accuracy_epoch': 0.7365385666996443}\n{'validation_loss_epoch': 2.865010300198117, 'validation_accuracy_epoch': 0.7916666672036454}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  22  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.92526355081675, 'training_accuracy_epoch': 0.736346554593975}\n{'validation_loss_epoch': 2.868922658868738, 'validation_accuracy_epoch': 0.7908220726090509}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  23  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9176839225146236, 'training_accuracy_epoch': 0.7452614108721415}\n{'validation_loss_epoch': 2.8719433320535197, 'validation_accuracy_epoch': 0.7860360371099936}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  24  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.921078346213516, 'training_accuracy_epoch': 0.7397410577657272}\n{'validation_loss_epoch': 2.872181615313968, 'validation_accuracy_epoch': 0.7849099104468887}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>▁▂▅▅▄▃▆▅▄▄▆▃▅█▃▅▃▄▆▄▄▆▅▅▆▆▃▆▅▄▄▄▃▂▃▂▂▄▃▄</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>███████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss_epoch</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss_every_n_batches</td><td>█▇▄▄▂▄▂▄▃▂▃▃▂▄▂▃▂▁▃▃▂▂▂▂▃▃▃▃▃▂▃▃▂▂▂▁▃▂▁▂</td></tr><tr><td>validation_accuracy_epoch</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>validation_loss_epoch</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>0.73974</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>27.20497</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>0.00519</td></tr><tr><td>training_loss_epoch</td><td>2.92108</td></tr><tr><td>training_loss_every_n_batches</td><td>2.90943</td></tr><tr><td>validation_accuracy_epoch</td><td>0.78491</td></tr><tr><td>validation_loss_epoch</td><td>2.87218</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">faithful-sweep-3</strong> at: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/9wxm37b8' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/9wxm37b8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240208_155720-9wxm37b8/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x4w69yt1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.012042785104009154\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ResNet101\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113365877786743, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eb833eb58cd4ad6862e06f4fae7fc42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240208_165201-x4w69yt1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/x4w69yt1' target=\"_blank\">smart-sweep-4</a></strong> to <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/x4w69yt1' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/x4w69yt1</a>"},"metadata":{}},{"name":"stdout","text":"{'batch_size': 64, 'learning_rate': 0.012042785104009154, 'model': 'ResNet101', 'optimizer': 'adam'}\n*\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"configuraiton class  {'batch_size': 64, 'learning_rate': 0.012042785104009154, 'model': 'ResNet101', 'optimizer': 'adam'}\nloading parameters\nbatch\nlr\noptimizer\nloaded parameters\ngetting optimizer\nadam\ngot optimizer\nModel is on GPU\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  0  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6119200152319832, 'training_accuracy_epoch': 0.02195945945945946}\n{'validation_loss_epoch': 3.6118718197471216, 'validation_accuracy_epoch': 0.029331140416233165}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  1  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611632772394129, 'training_accuracy_epoch': 0.02218423058857789}\n{'validation_loss_epoch': 3.609579713721024, 'validation_accuracy_epoch': 0.03399122820088738}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  2  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611545704506539, 'training_accuracy_epoch': 0.022804054054054054}\n{'validation_loss_epoch': 3.6119825212579024, 'validation_accuracy_epoch': 0.02604166673202264}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  3  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611432574890755, 'training_accuracy_epoch': 0.025760135135135136}\n{'validation_loss_epoch': 3.6129696996588456, 'validation_accuracy_epoch': 0.02055921052631579}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  4  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6118753729639828, 'training_accuracy_epoch': 0.02366227112911843}\n{'validation_loss_epoch': 3.611271996247141, 'validation_accuracy_epoch': 0.02549342105263158}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  5  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611988911757598, 'training_accuracy_epoch': 0.02514031166965897}\n{'validation_loss_epoch': 3.614766735779612, 'validation_accuracy_epoch': 0.031524122937729486}\nEpoch 00006: reducing learning rate of group 0 to 6.0214e-03.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  6  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611342059599387, 'training_accuracy_epoch': 0.024295717075064376}\n{'validation_loss_epoch': 3.6093225855576363, 'validation_accuracy_epoch': 0.024396929889917374}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  7  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6104227336677344, 'training_accuracy_epoch': 0.02934966216216216}\n{'validation_loss_epoch': 3.6082035240374113, 'validation_accuracy_epoch': 0.030701754516676852}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  8  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6100248001717232, 'training_accuracy_epoch': 0.03127724496093956}\n{'validation_loss_epoch': 3.6115417354985286, 'validation_accuracy_epoch': 0.024396929889917374}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  9  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611212282567411, 'training_accuracy_epoch': 0.02533783783783784}\n{'validation_loss_epoch': 3.612285614013672, 'validation_accuracy_epoch': 0.02631578947368421}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  10  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6102843735669112, 'training_accuracy_epoch': 0.025971283783783782}\n{'validation_loss_epoch': 3.611092253735191, 'validation_accuracy_epoch': 0.030153508837285795}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  11  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6109553285547205, 'training_accuracy_epoch': 0.026407203561550862}\n{'validation_loss_epoch': 3.610472403074566, 'validation_accuracy_epoch': 0.029605263157894735}\nEpoch 00012: reducing learning rate of group 0 to 3.0107e-03.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  12  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.609855117024602, 'training_accuracy_epoch': 0.027462946804794105}\n{'validation_loss_epoch': 3.610241312729685, 'validation_accuracy_epoch': 0.03289473684210526}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  13  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611759015031763, 'training_accuracy_epoch': 0.025548986486486486}\n{'validation_loss_epoch': 3.609664352316605, 'validation_accuracy_epoch': 0.03645833346404528}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  14  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6109612406911076, 'training_accuracy_epoch': 0.028293918918918918}\n{'validation_loss_epoch': 3.6085063658262553, 'validation_accuracy_epoch': 0.03919956146886474}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  15  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6109785550349467, 'training_accuracy_epoch': 0.0278852441020914}\n{'validation_loss_epoch': 3.608272100749769, 'validation_accuracy_epoch': 0.034265350942548956}\nEpoch 00016: reducing learning rate of group 0 to 1.5053e-03.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  16  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6104997815312565, 'training_accuracy_epoch': 0.02725179815614546}\n{'validation_loss_epoch': 3.606795587037739, 'validation_accuracy_epoch': 0.03344298252149632}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  17  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6101985396565617, 'training_accuracy_epoch': 0.030643799014993617}\n{'validation_loss_epoch': 3.6106714825881157, 'validation_accuracy_epoch': 0.02549342105263158}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  18  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.610750208029876, 'training_accuracy_epoch': 0.029152135993983294}\n{'validation_loss_epoch': 3.609462763133802, 'validation_accuracy_epoch': 0.03508771936360158}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  19  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.611600814638911, 'training_accuracy_epoch': 0.02681587837837838}\n{'validation_loss_epoch': 3.610425635388023, 'validation_accuracy_epoch': 0.03810307030615054}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  20  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6106736692222388, 'training_accuracy_epoch': 0.02956081081081081}\n{'validation_loss_epoch': 3.608775414918598, 'validation_accuracy_epoch': 0.030153508837285795}\nEpoch 00021: reducing learning rate of group 0 to 7.5267e-04.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  21  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6100291142592558, 'training_accuracy_epoch': 0.02978558193992924}\n{'validation_loss_epoch': 3.609946326205605, 'validation_accuracy_epoch': 0.02055921052631579}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  22  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.610224159988197, 'training_accuracy_epoch': 0.02809639275074005}\n{'validation_loss_epoch': 3.6091987459283126, 'validation_accuracy_epoch': 0.03563596504299264}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  23  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.610078225264678, 'training_accuracy_epoch': 0.030616554054054054}\n{'validation_loss_epoch': 3.60930545706498, 'validation_accuracy_epoch': 0.03125}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  24  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.6098564476580233, 'training_accuracy_epoch': 0.02491554054054054}\n{'validation_loss_epoch': 3.6088812727677193, 'validation_accuracy_epoch': 0.03453947368421053}\nEpoch 00025: reducing learning rate of group 0 to 3.7634e-04.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>▁▁▂▄▂▃▃▇█▄▄▄▅▄▆▅▅█▆▅▇▇▆█▃</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>▃▁▇▂▁▄▅█▂▂▂▁▂▁▁▁▂▁▁▁▁▂▂▁▂▁▂▁▁▇▁▁▁▂▂▁▁▁▂▁</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>██████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss_epoch</td><td>█▇▇▆██▆▃▂▅▂▅▁▇▅▅▃▂▄▇▄▂▂▂▁</td></tr><tr><td>training_loss_every_n_batches</td><td>█▇▇▇▇▇▇█▁███▇▇▇██▇▇█▃▇█▇▇██▇▇█▇▅▇██▇▇█▁▅</td></tr><tr><td>validation_accuracy_epoch</td><td>▄▆▃▁▃▅▂▅▂▃▅▄▆▇█▆▆▃▆█▅▁▇▅▆</td></tr><tr><td>validation_loss_epoch</td><td>▅▃▆▆▅█▃▂▅▆▅▄▄▄▃▂▁▄▃▄▃▄▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>0.02492</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>0.02474</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>0.00075</td></tr><tr><td>training_loss_epoch</td><td>3.60986</td></tr><tr><td>training_loss_every_n_batches</td><td>3.61521</td></tr><tr><td>validation_accuracy_epoch</td><td>0.03454</td></tr><tr><td>validation_loss_epoch</td><td>3.60888</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">smart-sweep-4</strong> at: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/x4w69yt1' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/x4w69yt1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240208_165201-x4w69yt1/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zsfyruny with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.009035439827953242\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ResNet101\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240208_174635-zsfyruny</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/zsfyruny' target=\"_blank\">cool-sweep-5</a></strong> to <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/zsfyruny' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/zsfyruny</a>"},"metadata":{}},{"name":"stdout","text":"{'batch_size': 32, 'learning_rate': 0.009035439827953242, 'model': 'ResNet101', 'optimizer': 'sgd'}\n*\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"configuraiton class  {'batch_size': 32, 'learning_rate': 0.009035439827953242, 'model': 'ResNet101', 'optimizer': 'sgd'}\nloading parameters\nbatch\nlr\noptimizer\nloaded parameters\ngetting optimizer\nsgd\ngot optimizer\nModel is on GPU\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  0  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.5176165833765145, 'training_accuracy_epoch': 0.17477918583519605}\n{'validation_loss_epoch': 3.278077576611493, 'validation_accuracy_epoch': 0.44313063089912}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  1  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.22789512686178, 'training_accuracy_epoch': 0.46376453775937865}\n{'validation_loss_epoch': 3.073974248525259, 'validation_accuracy_epoch': 0.589527027027027}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  2  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.148214808937644, 'training_accuracy_epoch': 0.5241524029751213}\n{'validation_loss_epoch': 3.0424851533528923, 'validation_accuracy_epoch': 0.6210585590955373}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  3  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.134809046375508, 'training_accuracy_epoch': 0.5311677090975703}\n{'validation_loss_epoch': 3.045514068088016, 'validation_accuracy_epoch': 0.6171171181910747}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  4  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.1097419456559785, 'training_accuracy_epoch': 0.5586117510081959}\n{'validation_loss_epoch': 3.008416188729776, 'validation_accuracy_epoch': 0.6585022525207417}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  5  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.077968284386356, 'training_accuracy_epoch': 0.5902526329974739}\n{'validation_loss_epoch': 2.992942172127801, 'validation_accuracy_epoch': 0.6728603614343179}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  6  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.0703259698387715, 'training_accuracy_epoch': 0.5945249066060904}\n{'validation_loss_epoch': 2.987519998808165, 'validation_accuracy_epoch': 0.6751126131495914}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  7  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.0618920910115146, 'training_accuracy_epoch': 0.6019585254241009}\n{'validation_loss_epoch': 2.9563371619662724, 'validation_accuracy_epoch': 0.7074887398126963}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  8  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.042991338133001, 'training_accuracy_epoch': 0.6196030832472301}\n{'validation_loss_epoch': 2.937048203236348, 'validation_accuracy_epoch': 0.7286036046775611}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  9  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.012032765109523, 'training_accuracy_epoch': 0.6563734363536445}\n{'validation_loss_epoch': 2.915843860523121, 'validation_accuracy_epoch': 0.7514076587316152}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  10  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.977297542857475, 'training_accuracy_epoch': 0.6919025673347265}\n{'validation_loss_epoch': 2.894456792522121, 'validation_accuracy_epoch': 0.7713963969333751}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  11  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.968694980452661, 'training_accuracy_epoch': 0.6978206602083582}\n{'validation_loss_epoch': 2.886096187540003, 'validation_accuracy_epoch': 0.780968469542426}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  12  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9561065346205315, 'training_accuracy_epoch': 0.7163978490699716}\n{'validation_loss_epoch': 2.8610542336025753, 'validation_accuracy_epoch': 0.8085585590955373}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  13  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9292448394152584, 'training_accuracy_epoch': 0.7386849899681247}\n{'validation_loss_epoch': 2.8469877178604537, 'validation_accuracy_epoch': 0.8181306317045882}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  14  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9439223445191676, 'training_accuracy_epoch': 0.7223296574994821}\n{'validation_loss_epoch': 2.8565170893798, 'validation_accuracy_epoch': 0.8077139645009428}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  15  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.940802911511895, 'training_accuracy_epoch': 0.7250589748629096}\n{'validation_loss_epoch': 2.8425647890245593, 'validation_accuracy_epoch': 0.8243243243243243}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  16  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.927884374346052, 'training_accuracy_epoch': 0.7376220647980567}\n{'validation_loss_epoch': 2.846221227903624, 'validation_accuracy_epoch': 0.816722972972973}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  17  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.914545865285964, 'training_accuracy_epoch': 0.7476135613966961}\n{'validation_loss_epoch': 2.833191040399912, 'validation_accuracy_epoch': 0.8271396401766185}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  18  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9171558010334873, 'training_accuracy_epoch': 0.7469826636671208}\n{'validation_loss_epoch': 2.8301249839164115, 'validation_accuracy_epoch': 0.8296734239604022}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  19  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.918268398362763, 'training_accuracy_epoch': 0.7454945684290256}\n{'validation_loss_epoch': 2.8358633196031726, 'validation_accuracy_epoch': 0.827984234771213}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  20  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9118606800935707, 'training_accuracy_epoch': 0.7525098745514747}\n{'validation_loss_epoch': 2.846374647037403, 'validation_accuracy_epoch': 0.8158783783783784}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  21  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.919253477433912, 'training_accuracy_epoch': 0.7452614108721415}\n{'validation_loss_epoch': 2.8331817614065633, 'validation_accuracy_epoch': 0.8341779290018855}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  22  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9100261399535094, 'training_accuracy_epoch': 0.7531407722810499}\n{'validation_loss_epoch': 2.8464996556977966, 'validation_accuracy_epoch': 0.8150337837837838}\nEpoch 00023: reducing learning rate of group 0 to 4.5177e-03.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  23  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.8855794481679697, 'training_accuracy_epoch': 0.7767582839849044}\n{'validation_loss_epoch': 2.8359108937753215, 'validation_accuracy_epoch': 0.8254504509874292}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  24  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.8877151888244006, 'training_accuracy_epoch': 0.7756542129581477}\n{'validation_loss_epoch': 2.8225446069562756, 'validation_accuracy_epoch': 0.8384009019748585}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>▁▄▅▅▅▆▆▆▆▇▇▇▇█▇▇█████████</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>▁▂▆██▇▄▆▄███▂█▄▆▄▅█▅▅▅▄▄▅▅▅▇▄▇█▂▇▅█▆▅▄█▇</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>█████████████████████████████████████▁▁▁</td></tr><tr><td>training_loss_epoch</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss_every_n_batches</td><td>█▇▄▅▄▃▅▄▄▄▄▅▄▄▄▄▄▃▃▃▂▂▃▃▃▃▃▃▂▃▃▃▃▂▄▃▃▁▂▂</td></tr><tr><td>validation_accuracy_epoch</td><td>▁▄▄▄▅▅▅▆▆▆▇▇▇█▇██████████</td></tr><tr><td>validation_loss_epoch</td><td>█▅▄▄▄▄▄▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy_epoch</td><td>0.77565</td></tr><tr><td>training_gradient_magnitude_every_n_batches</td><td>4.16989</td></tr><tr><td>training_learning_rate_every_n_batches</td><td>0.00452</td></tr><tr><td>training_loss_epoch</td><td>2.88772</td></tr><tr><td>training_loss_every_n_batches</td><td>2.9748</td></tr><tr><td>validation_accuracy_epoch</td><td>0.8384</td></tr><tr><td>validation_loss_epoch</td><td>2.82254</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cool-sweep-5</strong> at: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/zsfyruny' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/zsfyruny</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240208_174635-zsfyruny/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8qoz1inu with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0055350048665050635\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: ResNet101\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240208_184311-8qoz1inu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/8qoz1inu' target=\"_blank\">likely-sweep-6</a></strong> to <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/sweeps/01tf0bop</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/8qoz1inu' target=\"_blank\">https://wandb.ai/jerze/cats%26dogs_ML%26DL_project/runs/8qoz1inu</a>"},"metadata":{}},{"name":"stdout","text":"{'batch_size': 32, 'learning_rate': 0.0055350048665050635, 'model': 'ResNet101', 'optimizer': 'sgd'}\n*\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"configuraiton class  {'batch_size': 32, 'learning_rate': 0.0055350048665050635, 'model': 'ResNet101', 'optimizer': 'sgd'}\nloading parameters\nbatch\nlr\noptimizer\nloaded parameters\ngetting optimizer\nsgd\ngot optimizer\nModel is on GPU\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  0  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.557079719037426, 'training_accuracy_epoch': 0.12929284613148695}\n{'validation_loss_epoch': 3.4105348587036133, 'validation_accuracy_epoch': 0.31221846873695785}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  1  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.326913911469129, 'training_accuracy_epoch': 0.37401936573236166}\n{'validation_loss_epoch': 3.1764236269770443, 'validation_accuracy_epoch': 0.5047860371099936}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  2  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.2027716669095616, 'training_accuracy_epoch': 0.47500411446402674}\n{'validation_loss_epoch': 3.0829528537956445, 'validation_accuracy_epoch': 0.6013513513513513}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  3  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.134913682937622, 'training_accuracy_epoch': 0.5449651633801104}\n{'validation_loss_epoch': 3.022453327436705, 'validation_accuracy_epoch': 0.655968469542426}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  4  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.0713725041370004, 'training_accuracy_epoch': 0.6106882265635899}\n{'validation_loss_epoch': 2.958535503696751, 'validation_accuracy_epoch': 0.7204391891891891}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  5  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 3.0124326190169977, 'training_accuracy_epoch': 0.6691491109173314}\n{'validation_loss_epoch': 2.9132743461711987, 'validation_accuracy_epoch': 0.7584459459459459}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  6  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9809400029733877, 'training_accuracy_epoch': 0.6940147025244576}\n{'validation_loss_epoch': 2.883684796255988, 'validation_accuracy_epoch': 0.7885698208937774}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  7  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9697891076405845, 'training_accuracy_epoch': 0.7040130564955627}\n{'validation_loss_epoch': 2.884931751199671, 'validation_accuracy_epoch': 0.7865990996360779}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  8  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9474889567109193, 'training_accuracy_epoch': 0.7259024576264985}\n{'validation_loss_epoch': 2.8545843781651676, 'validation_accuracy_epoch': 0.8178490996360779}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  9  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9408201327940233, 'training_accuracy_epoch': 0.7333840792681895}\n{'validation_loss_epoch': 2.842897370054915, 'validation_accuracy_epoch': 0.8262950455820238}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  10  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9308659365388, 'training_accuracy_epoch': 0.7393364602205704}\n{'validation_loss_epoch': 2.841807139886392, 'validation_accuracy_epoch': 0.8215090100829666}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  11  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.932527329646, 'training_accuracy_epoch': 0.7378620801328801}\n{'validation_loss_epoch': 2.841548133540798, 'validation_accuracy_epoch': 0.8265765776505342}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  12  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.920192240046806, 'training_accuracy_epoch': 0.7444179285140264}\n{'validation_loss_epoch': 2.846952083948496, 'validation_accuracy_epoch': 0.8161599104468887}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  13  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.9069091251918246, 'training_accuracy_epoch': 0.7641951940497573}\n{'validation_loss_epoch': 2.827518598453419, 'validation_accuracy_epoch': 0.8381193699063482}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  14  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.918509510909619, 'training_accuracy_epoch': 0.7493005265184001}\n{'validation_loss_epoch': 2.8273971016342574, 'validation_accuracy_epoch': 0.8353040540540541}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  15  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.896327033334849, 'training_accuracy_epoch': 0.7705796024426311}\n{'validation_loss_epoch': 2.8355609146324365, 'validation_accuracy_epoch': 0.8316441452181017}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  16  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.894501588782486, 'training_accuracy_epoch': 0.7709910577657272}\n{'validation_loss_epoch': 2.8275789054664404, 'validation_accuracy_epoch': 0.8347409915279698}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  17  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.888826084785721, 'training_accuracy_epoch': 0.7750370307844512}\n{'validation_loss_epoch': 2.8302549542607487, 'validation_accuracy_epoch': 0.8336148648648649}\nEpoch 00018: reducing learning rate of group 0 to 2.7675e-03.\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  18  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.878849644239257, 'training_accuracy_epoch': 0.7833209897385163}\n{'validation_loss_epoch': 2.820335053108834, 'validation_accuracy_epoch': 0.84375}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n~~~~~~~~~~~~~~~~~~~~~ Epoch:  19  ~~~~~~~~~~~~~~~~~~~~~\n{'training_loss_epoch': 2.8708299033495845, 'training_accuracy_epoch': 0.7926953037579855}\n{'validation_loss_epoch': 2.8068737661516345, 'validation_accuracy_epoch': 0.8555743243243243}\nimage has Alpha channel\nimage has Alpha channel\nimage has Alpha channel\n","output_type":"stream"}]},{"cell_type":"code","source":"manager = TrainingManager()\nmanager.training(traning_config, annotations)","metadata":{"_uuid":"885def7c-54e8-4459-8c58-419ac4060d36","_cell_guid":"7974534f-ee8a-4830-8f53-92dd1d201fff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n#optimizer = optim.Adam(model.parameters(), lr=0.0001)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True, min_lr=5e-4, factor=0.3)\nconfig = configuration(batch_size, model, optimizer, scheduler, preprocess_transforms, costum_transforms)\nprint(config.get_configuration_info())","metadata":{"_uuid":"618e6531-b772-4f69-9ca0-6d93cf00a517","_cell_guid":"ca2fba95-7176-48cb-bf57-d22c3bd86456","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initialize_wandb(config)","metadata":{"_uuid":"98625201-3f41-4da9-bdff-09f624af9ba4","_cell_guid":"c5e62387-3f8e-4c66-ad6d-4e4cdba7f3fb","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"manager = TrainingManager()\nmanager.training(config, annotations)","metadata":{"_uuid":"9f17e995-d40b-48bc-8d18-e71bbf4f4f82","_cell_guid":"21f71aa4-7b66-40f6-9fe9-1070be4d0505","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config.last_checkpoint_path = \"/kaggle/input/checkpoint/21_ResNet_SGD_Batch64_lr0.01.pth\"\nmanager = TrainingManager()\nacc, confusion_mat = manager.evaluate_on_test_dataset(config, annotations)\nprint(acc)","metadata":{"_uuid":"60502f16-c14c-42cb-85bf-e547a99db699","_cell_guid":"27e44661-9807-47be-9a0b-050a849cae6d","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_mat)","metadata":{"_uuid":"e29fa0e9-696f-41fa-8059-f39514f76818","_cell_guid":"b8c78d58-49d9-4f7a-a7ed-8edc17f08616","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_on_test_dataset(config, annotations):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        #setup_configuration(self, config, annotations)\n        #self.net.eval()\n        config.model.eval()\n        train_loader, valid_loader, test_loader = get_dataloaders(annotations, config)\n        predictions = []\n        labels = []\n        with torch.no_grad():\n            for inputs_batch, labels_batch in test_loader:\n                inputs_batch = inputs_batch.to(device)\n                labels_batch = labels_batch.tolist()\n                outputs = model(inputs_batch)\n                predicted_classes = [torch.argmax(pred).item() for pred in outputs]\n                predicted_classes = predicted_classes\n                predictions += predicted_classes\n                labels += labels_batch\n            #print(predictions)\n            #print(labels)\n            confusion_mat = confusion_matrix(labels, predictions)\n        print(accuracy_score(labels, predictions))\n        print(confusion_mat)","metadata":{"_uuid":"8c63a8e6-1326-4225-a11d-25d9de3ce0fe","_cell_guid":"f5a6b487-62b2-4b0d-b3e7-b6e7756f19bb","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_on_test_dataset(config,annotations)","metadata":{"_uuid":"2e4cbd2d-07e2-4ab7-a01c-5e27c786ce82","_cell_guid":"a546e78c-0c69-4878-942d-2c143b2b9024","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ResNet18","metadata":{"_uuid":"0149dd9a-0ed2-4d7a-b2fc-d7c0aeb6b181","_cell_guid":"c719a467-8d42-4e9d-9d97-ba5297f7a5d9","trusted":true}},{"cell_type":"markdown","source":"resnet18model = models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n\nnum_classes = 37\nresnet18model.fc = nn.Sequential(\n    nn.Linear(resnet18model.fc.in_features, num_classes),\n    nn.Softmax(dim=1)\n)\nnn.init.xavier_normal_(resnet18model.fc[0].weight)\nnn.init.constant_(resnet18model.fc[0].bias, 0)","metadata":{"_uuid":"54181a10-1604-401f-ab99-df0ced7170cf","_cell_guid":"3574e3ba-116d-4b8d-8e76-8c8f2d1444f6","execution":{"iopub.status.busy":"2024-01-28T21:50:55.233334Z","iopub.status.idle":"2024-01-28T21:50:55.233660Z","shell.execute_reply.started":"2024-01-28T21:50:55.233500Z","shell.execute_reply":"2024-01-28T21:50:55.233516Z"},"trusted":true}},{"cell_type":"markdown","source":"costum_transforms = torchvision.transforms.Compose([\n    #transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(p=0.4),\n  ])\nweights = torchvision.models.ResNet18_Weights.DEFAULT\npreprocess_transforms = weights.transforms()","metadata":{"_uuid":"39f13cff-1e9b-40f4-a3a7-47f5cc44b2b5","_cell_guid":"f61dc69c-c4a7-4bfe-b405-cc1e850ff0de","execution":{"iopub.status.busy":"2024-01-28T21:50:55.235245Z","iopub.status.idle":"2024-01-28T21:50:55.235573Z","shell.execute_reply.started":"2024-01-28T21:50:55.235414Z","shell.execute_reply":"2024-01-28T21:50:55.235429Z"},"trusted":true}},{"cell_type":"markdown","source":"batch_size = 64\n#optimizer = torch.optim.SGD(resnet18model.parameters(), lr=0.8)\noptimizer = optim.Adam(resnet18model.parameters(), lr=0.0001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True, min_lr=7e-6, factor=0.5)\nconfig = configuration(batch_size, resnet18model, optimizer, scheduler, preprocess_transforms, costum_transforms)\nprint(config.get_configuration_info())","metadata":{"_uuid":"4d207070-7acf-473a-93f3-cbc7e79f19db","_cell_guid":"f64b4833-5bab-43e0-acd4-f50bd7f1acba","execution":{"iopub.status.busy":"2024-01-28T21:50:55.236651Z","iopub.status.idle":"2024-01-28T21:50:55.236951Z","shell.execute_reply.started":"2024-01-28T21:50:55.236800Z","shell.execute_reply":"2024-01-28T21:50:55.236814Z"},"trusted":true}},{"cell_type":"markdown","source":"config.get_configuration_dictionary()","metadata":{"_uuid":"667b51cd-9625-4bde-bebb-6c21a6137d30","_cell_guid":"7b477871-a876-4035-807f-8eaa13a62065","execution":{"iopub.status.busy":"2024-01-28T21:50:55.238671Z","iopub.status.idle":"2024-01-28T21:50:55.239118Z","shell.execute_reply.started":"2024-01-28T21:50:55.238885Z","shell.execute_reply":"2024-01-28T21:50:55.238906Z"},"trusted":true}},{"cell_type":"markdown","source":"initialize_wandb(config)","metadata":{"_uuid":"85a36426-ab75-4df1-ba4e-6758a6558bbd","_cell_guid":"3f265652-2cb3-4d2c-9264-24cf14ec8077","execution":{"iopub.status.busy":"2024-01-28T21:50:55.240486Z","iopub.status.idle":"2024-01-28T21:50:55.240935Z","shell.execute_reply.started":"2024-01-28T21:50:55.240707Z","shell.execute_reply":"2024-01-28T21:50:55.240729Z"},"trusted":true}},{"cell_type":"markdown","source":"manager = TrainingManager()\nmanager.training(config, annotations)","metadata":{"_uuid":"a4e1bd2e-bee5-472a-9fcb-1361c07e7cdb","_cell_guid":"decc90ee-9b35-48ef-af9b-776939b022d2","execution":{"iopub.status.busy":"2024-01-28T21:50:55.242090Z","iopub.status.idle":"2024-01-28T21:50:55.242550Z","shell.execute_reply.started":"2024-01-28T21:50:55.242320Z","shell.execute_reply":"2024-01-28T21:50:55.242343Z"},"trusted":true}},{"cell_type":"markdown","source":"mobileNet_model = torchvision.models.mobilenet_v2(pretrained=True)\npreprocess_transforms = mobileNet_model.transforms()\n\nnum_classes = 37\nmobileNet_model.classifier[1] = nn.Sequential(\n    nn.Linear(mobileNet_model.last_channel, num_classes),\n    nn.Softmax(dim=1)\n)\nmobileNet_model.classifier[1][0].apply(initialize_weights)","metadata":{"_uuid":"c4655ba1-b3b9-4a67-983e-a0c856d6f653","_cell_guid":"433ea531-b02c-4aa7-8075-c08e8b022133","execution":{"iopub.status.busy":"2024-01-28T21:50:55.243821Z","iopub.status.idle":"2024-01-28T21:50:55.244281Z","shell.execute_reply.started":"2024-01-28T21:50:55.244036Z","shell.execute_reply":"2024-01-28T21:50:55.244057Z"},"trusted":true}},{"cell_type":"markdown","source":"batch_size = 64\noptimizer = torch.optim.SGD(resnet18model.parameters(), lr=0.000001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=20, verbose=True, min_lr=7e-5, factor=0.5)\nconfig2 = configuration(batch_size, mobileNet_model, optimizer, scheduler, preprocess_transforms, costum_transforms)\n\nprint(config2.get_configuration_info())","metadata":{"_uuid":"0b6bf8be-f101-4abb-82ee-366b77137f43","_cell_guid":"51ba2bf8-270f-4ba0-9f73-b2df9c6a790b","execution":{"iopub.status.busy":"2024-01-28T21:50:55.245391Z","iopub.status.idle":"2024-01-28T21:50:55.245837Z","shell.execute_reply.started":"2024-01-28T21:50:55.245602Z","shell.execute_reply":"2024-01-28T21:50:55.245623Z"},"trusted":true}},{"cell_type":"markdown","source":"preprocess_transforms","metadata":{"_uuid":"d39759e4-b234-4088-bdd3-b35773c0c2f2","_cell_guid":"2dc77120-ab80-482e-af88-84b7fef69b43","execution":{"iopub.status.busy":"2024-01-28T21:50:55.246835Z","iopub.status.idle":"2024-01-28T21:50:55.247288Z","shell.execute_reply.started":"2024-01-28T21:50:55.247043Z","shell.execute_reply":"2024-01-28T21:50:55.247066Z"},"trusted":true}},{"cell_type":"code","source":"","metadata":{"_uuid":"b82cfa4b-c6ab-4e89-bd17-1a0dfc0dbf44","_cell_guid":"6c3edf04-2c13-4b8e-8821-39ef1c639c39","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"manager = TrainingManager()\nmanager.training(config2, annotations)","metadata":{"_uuid":"e80aaeee-e176-4d84-9bdc-f86836b67739","_cell_guid":"87001051-235c-42c2-b448-6d71fbfb5094","execution":{"iopub.status.busy":"2024-01-28T21:50:55.248588Z","iopub.status.idle":"2024-01-28T21:50:55.249033Z","shell.execute_reply.started":"2024-01-28T21:50:55.248810Z","shell.execute_reply":"2024-01-28T21:50:55.248831Z"},"trusted":true}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"6a072d61-77be-4feb-8445-139974e52926","_cell_guid":"de911ca3-e130-4950-9722-f065db05c33a","trusted":true}}]}